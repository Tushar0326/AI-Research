{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6e5c8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eaff25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([5, 8])\n",
      "Q shape: torch.Size([5, 8])\n",
      "K shape: torch.Size([5, 8])\n",
      "V shape: torch.Size([5, 8])\n",
      "\n",
      "In self-attention, Q, K, V all come from the same input x!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Create Q, K, V from input\n",
    "seq_len, d_model = 5, 8\n",
    "\n",
    "# Input sequence (e.g., word embeddings)\n",
    "x = torch.randn(seq_len, d_model)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "# Linear transformations to create Q, K, V\n",
    "W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "Q = W_q(x)  # Query\n",
    "K = W_k(x)  # Key\n",
    "V = W_v(x)  # Value\n",
    "\n",
    "print(f\"Q shape: {Q.shape}\")\n",
    "print(f\"K shape: {K.shape}\")\n",
    "print(f\"V shape: {V.shape}\")\n",
    "print(\"\\nIn self-attention, Q, K, V all come from the same input x!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e950847e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10, 64])\n",
      "Output shape: torch.Size([2, 10, 64])\n",
      "Attention weights shape: torch.Size([2, 10, 10])\n",
      "\n",
      "Attention weights for first sequence, first position:\n",
      "tensor([0.1140, 0.0996, 0.1103, 0.1119, 0.0694, 0.1116, 0.0957, 0.0946, 0.0731,\n",
      "        0.1198], grad_fn=<SelectBackward0>)\n",
      "Sum: 1.000 (should be 1.0)\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"Self-attention layer from scratch\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Linear layers to create Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor [batch_size, seq_len, d_model]\n",
    "        Returns:\n",
    "            output: [batch_size, seq_len, d_model]\n",
    "            attention_weights: [batch_size, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Create Q, K, V\n",
    "        Q = self.W_q(x)  # [batch_size, seq_len, d_model]\n",
    "        K = self.W_k(x)  # [batch_size, seq_len, d_model]\n",
    "        V = self.W_v(x)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Compute attention scores: QK^T\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1))  # [batch_size, seq_len, seq_len]\n",
    "        \n",
    "        # Scale by sqrt(d_model)\n",
    "        scores = scores / np.sqrt(d_model)\n",
    "          # Softmax to get attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)  # [batch_size, seq_len, seq_len]\n",
    "        \n",
    "        # Apply attention to values\n",
    "        output = torch.matmul(attention_weights, V)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test the self-attention layer\n",
    "d_model = 64\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "self_attn = SelfAttention(d_model)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, attn_weights = self_attn(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"\\nAttention weights for first sequence, first position:\")\n",
    "print(attn_weights[0, 0])\n",
    "print(f\"Sum: {attn_weights[0, 0].sum():.3f} (should be 1.0)\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a0b498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
